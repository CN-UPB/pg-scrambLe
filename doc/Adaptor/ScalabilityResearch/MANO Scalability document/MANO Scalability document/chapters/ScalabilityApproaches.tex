\chapter{Scalability Approaches}
\label{ch:Scalability Approaches}

Some of the scaling approaches from various academic work are discussed in this chapter. In each section a brief introduction about each  scalability approach and the relavance of a specific approach to our context of research (MANO scalability) has been discussed.

\section{Service replication}

A technique to clone services running on other nodes to stabilize the service load among different nodes without causing any damage to the ongoing operations. Services that are replicated secure additional resources provided by the new nodes for handling larger service load. In other words, service replication enhances service scalability and reduces the risk of QoS degradation by handling larger service loads. In a case study, \cite{falatah_cloud_2014} Falatah and Omar performed an analysis by varying the system load.Firstly, a variable for service load is set. The service load is a number of service invocations within a unit time. For the case study, the unit time was set to be 500ms. That is, if ten invocations occur within 500ms, then the service load is 10. To show an effectiveness of service replication, they simulate the service replication scheme for seventeen different volumes of service load. On each service load, conventional service system is compared with service replication strategy in terms of average response time.

In terms of MANO, service replication is easier when the server is stateless, but the MANO will have a database, user session and various managed services. Simply replicating servers cannot be the solution, multiple MANO using a single database will lead to a bottleneck. Hence database clustering or database replication is also needed while maintaining uniformity accross the databases.  
Service replication increases availability and parallelism.


\section{Service Migration}

Service migration is a strategy of placing a service to a different node when a particular node cannot provide high QoS due to a hardware/software problem, or the physical distance between consumers and providers. After service migration, the migrated service performs the same role that it was supposed to performed on the unstable node and the unstable node is removed from the list of service nodes.
The removal of this unstable node reduces overall QoS degradation \cite{lee_software_2010}.
The above authors conducted a simulation where the service was migrated to a node that is located closer to consumer. There is also an assumption made that the response time is directly proportional to the distance. Hence, a service is migrated to the fastest node in terms of response time.

In terms of MANO, they typically manage VIMs. There is a close association between MANOs and VIMs to get an NS instatiated.
Mere migration of a MANO server closer to the user will not make the communication time between MANO and VIM faster. Therefore, in MANOs case, it seems like service replication is a better strategy.


\section{Proactive scaling}

Proactive scaling also known as scheduled scaling is mostly done in a cloud by scaling at predictable, fixed intervals or when big traffic stream is anticipated. A well-designed proactive scaling system enables providers to schedule capacity alterations based on a plan.
With scheduled scaling, one can set when to increase the capacity or number of servers and when to decrease them. To implement proactive scaling, one should first understand expected traffic flow, which means the providers should have some kind of statistics which indicates the desired(usual) traffic, deviation (usually high) from expected traffic \cite{falatah_cloud_2014}\cite{reese_cloud_nodate}.This type of scaling is suitable for servers that will have increased load during known days 

In MANOs' terms, the MANO should be able to use these statistics and decide a scaling action. At the specified times, it should scale up with the values for normal, minimum or maximum traffic surges.


\section{Reactive scaling}

A reactive scaling strategy also known as auto-scaling adjusts its capacity by adding or removing, scaling up or down resources.
This type of scheme could be useful when the scheduled scaling plan goes wrong in proactive scaling. Cloud providers as well as cloud agencies require periodic acquisition of performance data for maintaining QoS. In addition, reactive scaling enables a provider to react quickly to unexpected demand. The crudest form of reactive scaling is utilization-based i.e. when  CPU, RAM  or some other resource reaches a certain level of  utilization, the provider adds  more of that resource to the environment\cite{falatah_cloud_2014}\cite{reese_cloud_nodate}. This type of scaling is suitable for servers that will have increased load during few unknown days.

In MANOs' terms, the plan is to develop a software called as "scalability manager" that could be installed in every MANO which helps scaling more children MANOs. It is the responsibilty of the scalability manager to scale MANOs and redirect the requests to the child MANO. This is done when the NS instances that are assigned to a particular MANO reaches the threshold.


\section{Predictive scaling}
This type of scaling uses machine learning to predict the traffic stream of a server/application beforehand so that the capacity changes can be done accordingly, It collects data from all the VM instances and various other data points and uses well trained machine learning models to actually predict the flow of traffic. This model would make use of one day's data and then the data is re-evaluated every 24 hours. This type of scaling strategy will be useful where servers are affected with cyclic periodic loads.

In terms of MANO, this type of scaling uses data from MANO instances and predicts the load on the server with the help of a machine learning model. This strategy is yet to be explored more to incorporate this in MANO.


\section{Service System Scaling - Dynamic node scaling}

To scale a service system across different nodes where these nodes are distributed all over the Internet, management components which help in monitoring the status of all these nodes are needed. These components help consumers manage the nodes as consumers would be unaware of the physical location of a service node that they use. Therefore, \cite{lee_software_2010} propose two key components to manage service scalability such as Global Scalability Manager (GSM) and Regional Scalability Manager (RSM).

The key role of GSM is to manage service scalability. It balances service load in the system by obtaining the current status of nodes that are listed in the service system and designs a scalability assuring method. Depending on this design, it directs any functionality.

RSM component is installed on all the service nodes. It observes the status of its node and transfers it to GSM. It also executes the scalability assuring scheme based on the instructions from GSM. These two components assist the service system to add or remove new nodes at run-time called as dynamic node management.

\subsection{Service Scalability Assuring Process}


\begin{itemize}
	

\item In this process, firstly define metrics for scalability measure. This metric could also be used to decide raw data to collect from services and to compute scalability in further steps.

\item Certain techniques from QoS monitored services are used to collect the set of raw data items \cite{Artaiam2008EnhancingSQ} \cite{hutchison_monitoring_2007}.

\item Next step is to analyze scalability metrics. If the metrics indicate an acceptable scalability level then continue step 2 and 3 where as if the metrics indicate a need to repair the below averaged scalalability then execute the following steps. 


\item Next up is to develop a remedy plan for improving the below avaraged scalability considering the current states of monitored service. Scalability assuring strategies like service replication and/or service migration could be adopted depending on the complexity and nature of the suffered service. 


\item In this step, the selected scalability strategy is executed and this is quite often an automated process. 

\item The final step is to inspect the result of the remedy plan and understand from the entire procedure as to how to improve the scalability. If the outcome of the process is valuable, then both the consequence and the remedy plan are logged for future uses thus making it a smart scalability framework.


\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/ServiceScalability}
	\caption{Service scalability Assuring Process from \cite{lee_software_2010}}
	\label{fig:servicescalability}
\end{figure}

\end{itemize}

Similarly In MANO's context, the metrics defined in section \ref{Metrics} could be the basis for deciding the scalabiliy. GSM and RSM can be developed as part of MANO's ecosystem. The scalability strategy can be a hybrid of the approaches discussed in this chapter.

\newpage
\section{Hierarchical Service Placement}
Service placement in a centralized network model lacks scalability. The service orchestrator in the centralized network model has a detailed view of all nodes/servers. A centralized placement algorithm maintaining all the information of all users and nodes is not a feasible approach, so the author investigates a hierarchical solution where the overall orchestration domain is split into geographical sub-domains.

In this model, the author refers a node as an Execution Zone (EZ - Services will be deployed in datacenters/clouds). The high-level orchestrator has limited visibility of EZ and user demands within a sub-domain - it sees only the aggregate of user demands and the aggregate of EZ capacities within a particular sub-domain. The high-level orchestrator places service instances at the coarse granularity of sub-domain only and subsequently each sub-domain orchestrator undertakes a further placement algorithm with the scope of that sub-domain only to determine in which specific EZs what quantity of service instances should be placed to supply the required number of session slots to meet the specific detailed demand pattern of user requests within that sub-domain \cite{maini_hierarchical_2016}.
\paragraph{}There are many ways of sub-dividing an overall orchestration domain into sub-domains. One option is to map sub-domains onto the same geographical area covered by resolution domains: the entity responsible for resolving user requests to EZs with available session slots. Another option is to consider multiple hierarchical levels of service orchestration and placement. The author models two levels of analysis.

The overview of this approach is that the lower-level domain is invisible to high-level orchestrator, the service placement problem can be subdivided into smaller units. One of them can be placed at the high level working at coarse granularity and the others: each per sub-domain can operate at a lower level with increased amount of information and decreased geographical exposure. In this way the optimisation algorithms can be executed with reduced quantities of information, increasing scalability. 
 
 
